# "Session crash after using up RAM" in Colab


---

**24ds1000042** on 2024-11-25:

Working on The “shortest path between actors” from Module 6 (in Colab).
How should we deal with RAM used up? 
@carlton @Jivraj

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/1)

---

**Jivraj** on 2024-11-25:

can you share code that you are using or share the notebook itself

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/2)

---

**24ds3000100** on 2024-11-26:

Hello @Jivraj
You may want to verify the following “Shortest path between actors” colab notebook:



colab.research.google.com



Google Colab





Please verify specifically the following read csv (I ran it a few times and the System RAM keep spiking till the 12.7 GB cap on the Colab free tier and crashes).
# Load the cast of each film. 2.0 GB RAM. 30s
cast = read_csv('title.principals.tsv.gz', sep='\t', na_values='\\N', dtype={
    'tconst': 'str',


[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/3)

---

**carlton** on 2024-11-26:

This is expected behaviour. The file is too large to load into a pandas dataframe with the RAM limitation of the free version of Google Colab

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/4)

---

**24ds3000100** on 2024-11-26:




 24ds3000100:

# Load the cast of each film. 2.0 GB RAM. 30s


Just curious though, the comment just above the customized csv read says, 2.0 GB RAM.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/5)

---

**24ds1000042** on 2024-11-26:

It is the "  shortest path " from Module 6 .(Notebook shared in module).
I’m told Colab Pro is needed to run it.
Am trying if I can work on a smaller dataset, to check the concepts in the module. Trying on the Bollywood datasets from Kaggle.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/6)

---

**siddhantbapna** on 2024-11-30:

@24ds3000100 @24ds1000042
you can use chunksize = 1024 in the param of of pd.read_xxx

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/7)

---

**24ds1000042** on 2024-12-01:

Thanks or the tip.
Did you try the standard read-csv or the one in the notebook( tweaked to deal with ‘trailing garbage’) ?
Just tried this on the tweaked one, somehow it seems to cause the output to be “TextFileReader” object. How did you work around that?

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/9)

---

**24ds1000042** on 2024-12-01:

I could not figure out a way to work around the error messages I got when using chunk size. Would be great if you could share how you managed.
Trying with ‘nrows’ in parameters worked. (got that from an old discourse post)
I could give upto nrows = 5000000 rows.(did not try beyond). Also did the following:

filtered for year of release of movies > 1977
filtered the other files to remove records that were not available in filtered movies .

So finally it ran without crashing.
image295×199 2.9 KB
@siddhantbapna @24ds3000100 @carlton

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/10)

---

**24ds3000100** on 2024-12-02:

[Assuming the options discussed in this thread or previous term’s threads do not resolve some part of the  problem with regards to the data. If they do, that’s great.]
I’d say, in the interest of time (project 2 commences today), the filtering approach is fine  since you were able to run the network path between actors examples that are towards the end of the notebook (which essentially demonstrates the concepts presented in the lecture).
You can debug, let’s say, post completion of project 2 - I am sure there are several interesting things to understand and learn for anyone interested in performance tuning (for example, memory profiling of the dataframes, why would a file read (disk size ~600MB) end up exhausting more thn 8 GB of RAM,  does data chunking help, what is the ‘trailing garbage’ problem being referred to in the comments etc.).

[Source](https://discourse.onlinedegree.iitm.ac.in/t/session-crash-after-using-up-ram-in-colab/157248/11)