# Request for Re-evaluation of Project 2


---

**22f3002811** on 2024-12-27:

Dear Tds team @carlton @jkmadathil @Bharathi @andrew ,
I hope this message finds you well. I would like to clarify my position and outline the key concerns regarding the evaluation of Project 2.
Firstly, let me emphasize that I have no issue with the discussion, learning objectives, or goals of the project. In fact, I thoroughly enjoy challenges and have appreciated this course more than any other in the diploma program. However, there is a clear distinction between engaging in academic exploration and the critical impact of grading, which directly influences our final results.
Now, addressing the core issue:
Attached below is a screenshot of the final evaluation results:
image1918×983 109 KB
These evaluation fields, which were hidden during the final evaluation, significantly affected (and, in my case, diminished) my final score. For instance:

ReadMe Evaluation:
How did the LLM determine that my ReadMe file lacked the necessary headings or structure? This judgment appears inconsistent and lacks transparency.
Chart Relevance:
How did the LLM evaluate the relevance of the charts in my submission? The charts I provided were entirely aligned with the project goals.
image1412×969 104 KB
For reference, my submission can be reviewed here: GitHub Repository.

To explore alternative evaluation methods, if peer review had been used, I believe it is reasonable to estimate that my final score would have ranged between 80-90%. This aligns with the initial evaluation script, which scored my submission at 89% (14/14).

This created a false sense of confidence for many students, myself included, that our scripts were adequately optimized and would receive a decent grade (as the evaluation script used llm to determine the final grade for test cases and it took a lot of time to take it from 4/14 to 14/14). This lack of transparency in the evaluation process led to confusion and ultimately to disappointment when the final grades were released.
Critical Concerns:

Transparency:
How do you justify the drastic reduction of a score from 89% (based on the initial evaluation) to a mere 41% in the final evaluation? This inconsistency undermines the credibility of the grading system.
Integrity:
The final evaluation process appears to have serious flaws. A peer review would likely have produced a score closer to 90%, which is far more representative of the effort and quality of the submission. The current system’s inability to provide consistent and reliable results is concerning.
Impact on Grades:
The abrupt decline in scores has directly impacted our final grade points, in some cases, reducing them from a ‘B’ to a ‘C.’ This disproportionately penalizes students and fails to reflect their genuine efforts.

Proposed Solutions:

Reevaluate all submissions using a new and fair evaluation script (though I understand this might not be feasible at this stage).
Implement a peer review system to provide more accurate and balanced grading.
Accept the initial test results (14/14 scores) as the final results. While this may be a basic measure, it is at least consistent and reflective of the criteria we optimized our scripts for.

What cannot be done, however, is ignoring the evident flaws in the evaluation system. Students invested weeks optimizing their scripts (from scores like 4/14 to 14/14) based on the visible tests, only to be blindsided by a system that ultimately did not function as intended.
While I am not opposed to new evaluation systems or changes, it is essential that such systems work transparently and consistently to ensure fairness for all students.
Thank you for your time and consideration. I look forward to your response.
Sincerely,
Aditya Gupta

[Source](https://discourse.onlinedegree.iitm.ac.in/t/request-for-re-evaluation-of-project-2/160668/1)