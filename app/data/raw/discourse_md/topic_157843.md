# Project 2 Discussion Thread [Sep 2024 Term]


---

**s.anand** on 2024-12-02:

TDS Project 2 is available at tools-in-data-science-public/project-2-automated-analysis.md at tds-2024-t3 · sanand0/tools-in-data-science-public · GitHub
It’s an interesting project.

You have to write an LLM-based program to analyze a dataset.
You have to convince an LLM that your code and output are good.

This is hard. It’s something you haven’t done before. (Nor most people in the world.)
So I strongly urge you to start early!
Please use this thread for any questions.
You’ll learn a lot of new things. Enjoy the journey!
UPDATE: 7 Dec 2024. The WIP evaluation script is at tools-in-data-science-public/project2 at tds-2024-t3-project2-wip · sanand0/tools-in-data-science-public · GitHub
UPDATE: 12 Dec 2024. Frequently Asked Questions

When is Project 2 due? Before Mon 16 Dec 2024,3:30 pm IST.
Where do I submit? https://forms.gle/r2sNJvGicxrhE9mYA
Can I use my own OpenAI API key? Yes
Where should I store the token? In the AI_PROXY environment variable. Don’t commit your token.
What if I ran out of the $2 credit on my token? We’ve increased it to $5 for everyone. You will not get more. Borrow from friends if required.
How do I check how much credit I have used so far? Every response has a monthlyCost field. Keep that below 5.0.
Won’t passing images to OpenAI cost too much? Use "detail": "low" to reduce it.
What should my folder structure be?

autolysis.py
goodreads/README.md
goodreads/*.png
happiness/README.md
happiness/*.png
media/README.md
media/*.png


How should I run my autolysis.py? uv run autolysis.py /path/to/input.csv
Where should I save the README.md and PNG files? In the current working directory.
How can I check my submission? Use the WIP Project 2 Evaluation Script.
Where are the results of this stored?

Linux/Mac: ~/.local/share/tds-sep-24-project-2/results.csv
Windows: %LOCALAPPDATA%\tds\tds-sep-24-project-2\results.csv


LLMs are error prone. Should my code retry on failure? Yes. E.g. use a library like tenacity.
Will you evaluate the code multiple times and take an average? No. Just once.
How long can my code run? Max 120 seconds.
What if I get CLONE FAILED ... returned non-zero exit status 128.? Your GitHub repo URL is wrong or your repo is private.
Which datasets will you evaluate? The project mentions “one of the datasets you submitted”. We’ll run uv run autolysis.py on either goodreads.csv, happiness.csv, or media.csv, randomly, and evaluate it. We will also evaluate it on 2 secret datasets.
I get a No module named .... Read the uv reading material
I get a HTTP 429 error. OpenAI is tired. Retry.
I get a different score each evaluation. LLMs give different results each time. Make your code and prompts more robust.

UPDATE: 14 Dec 2024. More FAQs

But my code works on my system… It needs to work via the evaluate script too. This project teaches you portable and deployable coding.
But it worked last time… APIs are unreliable. Learn how to handle that, e.g. via retries, caching, timeouts, etc.
But I got a different score last time… LLMs are not deterministic. Learn how to handle that, e.g. via robust prompts.
But my code times out… Shorten your prompts. Reduce the number of LLM requests. Pray for luck.
What’s the max total from the evaluation script? 14 when you run it. When I run it, it’ll add 2 extra datasets taking the total to 20. (Plus 8 bonus marks for originality and 4 for wow factor. Finally, normalization.)
What if the code evaluation gives me contradictory feedback? The final code evaluation script is a secret. It’ll be far more granular. Use the evaluation output as a guide but focus on writing good code.
How do I handle No module named ...? Read the Project 2 reading material on uv
Should I pip install my dependencies? No. Read the Project 2 reading material on uv
Where should I save my generated README.md and PNG files?. In the current directory, NOT the script directory, nor the data directory.
How do I handle Your authentication token is not from a valid issuer...? Use http://aiproxy.sanand.workers.dev/openai/v1 as your OPENAI_API_BASE when using the openai library with AIProxy
I have a Readme.md but the evaluation script script says I dont. File names will be evaluated case-sensitively. Create a README.md and *.png in exactly that case.


UPDATE: 15 Dec 2024. Second submission evaluation.
I’ve evaluated 343 Project 2 submissions starting around 4 pm IST today.
Of these, 151 got 3/3, and these are PROBABLY OK. Others may copy your code. You might improve & test it locally for uniqueness and push before the deadline.
The remaining 192 submissions are not OK.
The evaluation is based on this evaluation script: tools-in-data-science-public/project2 at tds-2024-t3 · sanand0/tools-in-data-science-public · GitHub
The results are at: TDS Sep 2024 - Project 2 Evaluation 2024-12-15 - Google Drive
results.csv has the status of the submission checks for each student. If your ID is not here, then you have not submitted with your IITM email. Please submit again.
logs/ has the logs, i.e. your code output (if there was any). I run the evaluator 2-3 times since I ran out of tokens. So some of you may see the output of two runs.
The top problems I see are:

Saving files in the script or data directory instead of the current working directory.
Not using script metadata for uv, or trying to install via pip
Script runs forever. NOT due to LLMs, but waiting for interactive visualizations, starting HTTP servers, etc.
AI Proxy token is not set up correctly.

These are classic deployment mistakes. Please fix them.
Some of your repos are private. I guess you know what you’re doing.
I won’t be able to run more evaluations before the deadline, nor revert on this thread, given travel. I suggest you run the evaluation script (preferably without Python in the path, with the data in a faraway directory) to check if your code works.
TDS 2024 Sep Project 2 Results
View Project 2 results.
This is the evaluation process.
Analyzing the results.csv will be interesting. Please do share your analysis and learnings.
FAQ

But I get different results locally. ANS: Your code ran differently in the test environment. LLMs and environments are not deterministic. Write code more robustly.
What’s fatal: could not read Username for 'https://github.com': terminal prompts disabled? ANS: Your repo is private.
What’s missing autolysis.py? ANS: Your script was not named autolysis.py and present in the root folder. Maybe it had a different name or was in a different folder.
Why’s my README.md or *.png missing? ANS: Your script didn’t generate them, or saved them in a wrong location.
Why do output: wage_theft.csv, output: house_rent.csv get 0 points? ANS: They are not part of the evaluation. They’re just an informational check.
What is code_hacking? ANS: The code has content that nudges the LLM towards higher code quality scores? This does not affect your score, but re-runs the evaluation.
What is output_hacking? ANS: The output has content that nudges the LLM towards higher output quality scores. That gives you a tiny bonus and re-runs the evaluation.
Why does it praise my score and still give me 0? ANS: The prompt instructs it to always tell you what’s good AND bad, irrespective of the score.
How was the code diversity bonus calculated? ANS: Using the % overlap (Jaccard similarity) with previous submissions. Bonus = Your marks * (1 - overlap) * 8 / 20
What about the “Engaging and interesting” bonus? ANS: None of the responses blew my mind. (Oh, there are very good ones, no doubt.) So I didn’t award any for this.

This concludes the Project 2 evaluation.
Fantastic work, everyone!!

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/1)

---

**s.anand** on 2024-12-02:



[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/2)

---

**24ds3000100** on 2024-12-02:

Hello Prof. Anand,
This is an interesting project.
@s.anand
A quick question -  students will remove their openapi token when they check in their code on github. Because their are many ways of externalizing and retrieving the api token, what should be the approach of retrieving the open api token in the checked in code?
Thank you.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/3)

---

**s.anand** on 2024-12-02:

Good point @24ds3000100 – please use the environment variable AIPROXY_TOKEN to store the token. Load the token from the environment variable and do not commit it. The evaluations will assume that autolysis.py will load the token from the environment variable AIPROXY_TOKEN.
I’ve updated the Project Docs.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/4)

---

**24f1002469** on 2024-12-02:

Thank you sir for details. As deadline is still not mentioned. Any tentative dates, I could plan my work accordingly.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/5)

---

**s.anand** on 2024-12-02:

@24f1002469 - thanks, I’ve added the due date to the project page. It is due by Thu 12 Dec 2024 EOD AoE.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/6)

---

**22f2000816** on 2024-12-03:

Few  question the Project 2.
I assume that auto evaluation will take the api kea as below and run the  xx.py file
os.environ[“AIPROXY_TOKEN”] = “api key”
uv run auto.py xx.csv
Is it right?
Is there a maximum number of PNG generated? Only up to 3 per csv file?

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/7)

---

**s.anand** on 2024-12-04:

@22f2000816 – yes, the auto evaluation will work like this:
export AIPROXY_TOKEN=[...]  # The app will use a new token
uv run autolysis.py /path/to/input.csv  # The filename must be autolysis.py and accept any CSV present in the file system

I recommend 3 PNGs per CSV. Going up to 5 might be OK, but keep it low. The entire output (including PNGs) must fit within GPT-4o-mini’s context window, and if it doesn’t, you’ll lose out on the evaluations, so best to test and ensure that your PNGs are under 512x512 px.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/8)

---

**Ammar_Sajjid** on 2024-12-04:

Sir can you share a bit more about how the evaluation will be carried out, I understand it will be done via an LLM but will you give it a supporting analysis done on the database by you(manually) [So it can compare the 2] or will the LLM just be given our analysis and be asked to grade it.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/9)

---

**s.anand** on 2024-12-04:

@Ammar_Sajjid - I will be sharing a partial evaluation script by Sunday.
The script will:

Directly check the Submission criteria
Pass your autolysis.py to an LLM with a prompt to evaluate each of the Code criteria
Run uv run autolysis.py dataset.csv
Pass the generated README.md and *.png to an LLM with a prompt to evaluate each of the Output criteria


[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/10)

---

**carlton** on 2024-12-04:

Out of curiosity more than anything else, the context window for GPT 4-o mini is 128k tokens. So even if we use a 2k image (8 tiles) that would mean 70 images would still only use up half the context window. Am i missing something?
Thanks

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/11)

---

**24ds1000075** on 2024-12-04:

import openai

AIPROXY_TOKEN = os.getenv("AIPROXY_TOKEN")
if not AIPROXY_TOKEN:
    print("Error: AIPROXY_TOKEN environment variable not set.")
    sys.exit(1)

# Initialize OpenAI client
openai.api_key = AIPROXY_TOKEN

response = openai.ChatCompletion.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "Hello project2"}
    ]
)
print(response)

For this API call, I am getting the below error.
AuthenticationError: Your authentication token is not from a valid issuer. Can you please help identifying the issue?
@carlton

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/12)

---

**carlton** on 2024-12-04:

You cannot use Open API directly. You have to send your crafted requests as shown in the lectures and the TA sessions to the AI Proxy whose documentation can be found here. You do the same as when you solved GA5.


github.com




GitHub - sanand0/aiproxy: Authorizing proxy for LLMs
Authorizing proxy for LLMs








[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/13)

---

**s.anand** on 2024-12-04:

That’s right, @carlton. But the system prompt I’ll be using to evaluate will be QUITE long, and I want to ensure I leave plenty of space for that.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/14)

---

**24ds3000100** on 2024-12-05:

Hello Prof. Anand,
If, as an option, autolysis.py executes python code that the LLM sends back (as an alternative to and/or in addition to function callbacks),  I presume there will be necessary checks in the evaluator to terminate the program should it take (way) too long to execute (say, the arbitrary code from the LLM has an infite loop) and/or hits some memory thresholds (among other factors).
Will the evaluator retry running the program a few times (should such scenarios occur) to see if it runs successfully in one of those retries? Or, is it just one execution - either success or failed case?
In regards to the code/generated artifacts review, will the score be a simple average of the score received from the LLM by sending the code/generated artifacts to the LLM a few times (because there may be (perhaps, slight) variations in the score and/or hallucinations, presumably)? Or, just one time invocation of the LLM?
Thank you.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/15)

---

**Ammar_Sajjid** on 2024-12-06:

I request, we add a feature to this project where students can check their script against, the evaluation script.
I understand it would be costly but we can restrict it down to 3 checks per student and also keep the checks limited to just the 3 databases already given to us, so we don’t get to check it against the unseen database.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/18)

---

**22f3001912** on 2024-12-06:

where and how do i submit this project cause in the portal am not seeing any page like the project 1

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/19)

---

**21f3002276** on 2024-12-06:

Here:


Google Docs



TDS Sep 2024 - Project 2 Submission
Please log in with your IITM email ID.







[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/21)

---

**23f2003124** on 2024-12-07:

I have some concerns regarding the TDS project. In order to analyze the data as specified, we are uncertain about the type of dataset we may encounter. Therefore, we must conduct a generic analysis that includes summary statistics, counting missing values, correlation matrices, outlier detection, clustering, and hierarchical detection, among other techniques. Additionally, it has been suggested to consult the language model (LLM) about the dataset by providing details such as the file name and column names to gain a better understanding of the dataset and to inquire about the types of analyses that can be performed on it.
My uncertainty lies in whether we will instruct the LLM to select the appropriate analysis methods from the functions we have predefined. For instance, a specific dataset may be effectively analyzed using correlation and time series analysis, and we have already established six to seven analytical functions, including these two. Will the LLM only recommend these two functions, or do we expect it to suggest the most suitable analyses for the dataset and also provide the corresponding code? If that is the case, how will we store the results generated by the code? For example, if it produces a correlation table with a heat map, how can we save that table for future narrative reporting and ensure that the generated heat map is saved as a PNG file? My research indicates that while I can generate and run the code instantaneously, I am unable to utilize the output for further analysis.
And, if we generate three or four charts, do we need to save all of them individually or combine them into a single image? This is particularly important as the requirements specify that there should be one image corresponding to each dataset.
Due to certain modification in doubt I have modified and reposted it

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/22)

---

**Ammar_Sajjid** on 2024-12-07:




 23f2003124:

My research indicates that while I can generate and run the code instantaneously, I am unable to utilize the output for further analysis.


I am assuming u are using exec() function to run the code generated by the LLM.
If there is any variable assignment within the code you access these variables once the code is generated.
image360×272 3.54 KB
Note: I am not saying this is the cleanest way to capture the output, but rather one of the ways.

[Source](https://discourse.onlinedegree.iitm.ac.in/t/project-2-discussion-thread-sep-2024-term/157843/23)